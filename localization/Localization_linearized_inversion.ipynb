{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29533eb",
   "metadata": {},
   "source": [
    "# Acoustic localization using linearized inversion\n",
    "\n",
    "## Purpose of this notebook\n",
    "The purpose of this notebook is to show all the processing steps and python commands needed to localize an acoustic signal in three-dimensions (3D) using linearized inversion. The example below uses data from the large audio-video array and corresponds to the localization results presented in figure 9 of the paper. Note that this notebook is only meant to be show the analysis process on a single example. For processing, a full deployment efficiently, the code should be slightly modified (i.e., removing all the plotting steps and looping through files automatically) and executed in a regular python file (.py). Details about the theory behind the detection and localization process can be found in the paper.\n",
    "\n",
    "The analysis process is composed of 6 steps:\n",
    "* Step 1: Importing all the necessary libraries\n",
    "* Step 2: Defining and loading the configuration files\n",
    "* Step 3: Finding data from all hydrophones\n",
    "* Step 4: Running the automatic detector\n",
    "* Step 5: Localizing the detected signals\n",
    "* Step 6: Saving the localization results\n",
    "\n",
    "All the data and config files needed for this notebook are in the folder [./localization/large-array](https://github.com/xaviermouy/XAV-arrays/tree/main/localization/large-array)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf637d46",
   "metadata": {},
   "source": [
    "## Step1: Importing all the necessary libraries\n",
    "The libraries used for the processing include ecosound, pandas and matplotlib (for plotting). All scripts used for the automatic detection and localization steps are in the python files detection.py and localization.py, respectively. The python file in tools.py contains other non-specific functions used throughout the processing.\n",
    "\n",
    "Before going through the notebook, you will need to install the library ecosound and ipympl. To do so, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ecosound --upgrade\n",
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d229da",
   "metadata": {},
   "source": [
    "If you run this notebook on Google Colab, you will need to execute the commands above at the beginning of each session. If you run this notebook localy on your own machine, **you only need to do this once**. Once all the libraries are installed, you can import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab13b18c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ecosound'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17260\\1585349251.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'widget'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mecosound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mecosound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDeploymentInfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mecosound\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ecosound'"
     ]
    }
   ],
   "source": [
    "%matplotlib widget \n",
    "import pandas as pd\n",
    "import ecosound.core.tools\n",
    "from ecosound.core.metadata import DeploymentInfo\n",
    "import ecosound\n",
    "import tools\n",
    "import detection\n",
    "import localization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c56c8c",
   "metadata": {},
   "source": [
    "## Step 2: Defining and loading the configuration files\n",
    "\n",
    "Before starting the processing we need to define four configuration files that define some metadata, describe the configuration of the array, and define the parameters to use for the detection and localization. Once these configuration files are defined, they can be used for processing the entire deployment.\n",
    "\n",
    "### Deployment metadata \n",
    "The first configration file is the deployment_info.csv. It defines the basic metadata about the deployment (recorder type, depths, coordinates, etc.). These metadata will be attached to the detection and localization results. Each line of the csv file define a separate channel/hydrophone. The field names should be relatively self-explanatory. To see the format to use, have a look at the example of deployment_info files used for the large array ([here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/large-array/deployment_info.csv)) and mobile array ([here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/mobile-array/deployment_info.csv)).\n",
    "\n",
    "You need to define which deployment_info file to use,then you can load it and inspect that all the fields are filled correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9aaac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# config file\n",
    "deployment_info_file = r'.\\large-array\\deployment_info.csv'\n",
    "\n",
    "# load and display deployment metadata\n",
    "Deployment = DeploymentInfo()\n",
    "Deployment.read(deployment_info_file)\n",
    "Deployment.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886da7e",
   "metadata": {},
   "source": [
    "### Hydrophones configuration\n",
    "\n",
    "The second configration file is the hydrophones_config csv file. It defines for each hydrophone, the coordinates, the channel the data is recorded to, and the location (path) of the data. See the examples of hydrophones_config file for the large array ([here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/large-array/hydrophones_config_07-HI.csv)) and the mobile array ([here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/mobile-array/hydrophones_config_HI-201909.csv)). Note that the first channel starts at 0.\n",
    "\n",
    "Let's define and load the hydrophones_config file for the large array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config file\n",
    "hydrophones_config_file = r'.\\large-array\\hydrophones_config_07-HI.csv'\n",
    "\n",
    "# load and plot hydrophone configuration\n",
    "hydrophones_config= pd.read_csv(hydrophones_config_file, skipinitialspace=True, dtype={'name': str, 'file_name_root': str}) # load hydrophone coordinates (meters)\n",
    "hydrophones_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594dc03",
   "metadata": {},
   "source": [
    "We can see that there is one row per hydrophone and 8 fields per row: \n",
    "\n",
    "* The **name** is a free field only used to label hydrophones in plots.\n",
    "* The **x**, **y**, and **z** define the 3D coordinates of the hydrophones in meters. \n",
    "* The **array_channel** identifies which channel the hydrophone corresponds to. It must be consistent with the **audio_channel_number** values identified in the deployment_info file. \n",
    "* The **audio_file_channel** identifies which channel is used in the audio file. Some recorders (like the AMAR recorders) save data from each channel in separate mono wav files. In such case, the **audio_file_channel** is set to 0 (since each file has only 1 channel). Other recorders (like the Soundtrap recorders) save data from all channels in a single audio file. In such case, the channel to use in the audio file must be identified. In the present case, we use data from the large array, which save data from each channel in a seperate mono file. So we set the \"audio_file_channel\" to 0 for all hydrophones. \n",
    "* **file_name_root** defines the first part of the filenames for each hydrophone. It is assumed that the remaining part of the audio file contains the time stamp of the file. For data from AMAR recorders, the audio file names start with the serial number of the AMAR, a dot, the channel number, a dot, and the timestamp. So, the **file_name_root** is different for each hydrophone. For Soundtraps recorders, data from all hydrophones are saved in a single multi-channel file, so **file_name_root** is the same for all hydrophones.\n",
    "\n",
    "To ensure that the hydrophone geometry makes sense, we can plot the hydrophoen locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = localization.plot_localizations3D(hydrophones=hydrophones_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046a3a6",
   "metadata": {},
   "source": [
    "### Detection parameters\n",
    "The third configuration file to define is the detection file. It is a .yaml files defining the different parameters used during the detection process. It includes the spectrogram, denoising and detections parameters. A description of each parameter can be found in the paper. An example of detection config file can be found [here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/large-array/detection_config_large_array.yaml).\n",
    "\n",
    "Let's define and load the detection config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5852911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config files\n",
    "detection_config_file = r'.\\large-array\\detection_config_large_array.yaml'\n",
    "\n",
    "# load configuration parameters\n",
    "detection_config = ecosound.core.tools.read_yaml(detection_config_file)\n",
    "detection_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541071bb",
   "metadata": {},
   "source": [
    "### localization parameters\n",
    "\n",
    "The last configuration file is the localization file (.yaml) that defines the different parameters for the linearized inversion. An example of localization config file can be found [here](https://github.com/xaviermouy/XAV-arrays/blob/main/localization/large-array/localization_config_large_array.yaml).\n",
    "\n",
    "Let's define and load the localization config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_config_file = r'.\\large-array\\localization_config_large_array.yaml'\n",
    "localization_config = ecosound.core.tools.read_yaml(localization_config_file)\n",
    "localization_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7cb3b",
   "metadata": {},
   "source": [
    "Each parameter is explained below:\n",
    "* **sound_speed_mps:** sound speed in the water, in meters per second\n",
    "* **ref_channel:** reference channel for the TDOA measurements\n",
    "* **upsample_res_sec:** time resolution desired for the cross correlation (in seconds)\n",
    "* **min_corr_val:**  minimum correlation values accepted for considering a TDOA measurement valid\n",
    "* **start_model:** coordinates of the first starting model to use in the linearized inversion (typically at teh center of the array, i.e. [0,0,0]).\n",
    "* **start_model_repeats:** number of times the linearized inversion is repeated using different starting models.\n",
    "* **damping_factor:** damping factor to use between iterations of the linearized inversion.\n",
    "* **stop_delta_m:** stoping criteria. Stops when the difference of model norms between two consecutive iterations is less than stop_delta_m.\n",
    "* **stop_max_iteration:** stoping criteria. Maximum number of iterations allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b4574",
   "metadata": {},
   "source": [
    "## Step 3: Finding data from all hydrophones\n",
    "\n",
    "We can now define the audio file we want to process. We can specify the name of a file from one hydrophone, and the code below will find out the corresponding files from the other hydrophones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c419fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file  to process\n",
    "infile = r'.\\large-array\\data\\AMAR173.4.20190920T163858Z.wav'\n",
    "\n",
    "# Look up data files for all channels\n",
    "audio_files = tools.find_audio_files(infile, hydrophones_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82307fd6",
   "metadata": {},
   "source": [
    "The variable **audio_files** now lists the path and name of the audio files from each hydrophone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f05ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d186cd0",
   "metadata": {},
   "source": [
    "We can then inspect the data from all channels by displaying the spectrograms and waveforms. The spectrogram parameters used for the spectrograms are the ones defined in the detection_config file defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847da4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all data \n",
    "graph_spectros, graph_waveforms = tools.plot_all_channels(audio_files,\n",
    "                                                          detection_config['SPECTROGRAM']['frame_sec'],\n",
    "                                                          detection_config['SPECTROGRAM']['window_type'],\n",
    "                                                          detection_config['SPECTROGRAM']['nfft_sec'],\n",
    "                                                          detection_config['SPECTROGRAM']['step_sec'],\n",
    "                                                          detection_config['SPECTROGRAM']['fmin_hz'],\n",
    "                                                          detection_config['SPECTROGRAM']['fmax_hz'],\n",
    "                                                          detections_channel=detection_config['AUDIO']['channel'],\n",
    "                                                          verbose=False)\n",
    "\n",
    "#graph_spectros.show(display=False)\n",
    "fig_graph_waveforms, ax_graph_waveforms = graph_waveforms.show(display=False)\n",
    "fig_graph_spectros, ax_graph_spectros = graph_spectros.show(display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595d96f",
   "metadata": {},
   "source": [
    "Stacked spectrograms for all hydrophones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_graph_spectros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5720a",
   "metadata": {},
   "source": [
    "Stacked waveforms for all hydrophones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_graph_waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66278c2",
   "metadata": {},
   "source": [
    "## Step 4: Running the automatic detector\n",
    "\n",
    "Teh detector can now be run on one of the hydrophones using the function **detection.run_detector**. The input arguments for that function are (in this order):\n",
    "1. The path of the file to process, \n",
    "2. The channel to use,\n",
    "3. the detection_config parameters (see step 2),\n",
    "4. the path of the deployment_info file (see step 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run detector on selected channel\n",
    "detections = detection.run_detector(audio_files['path'][detection_config['AUDIO']['channel']],\n",
    "                                    audio_files['channel'][detection_config['AUDIO']['channel']],\n",
    "                                    detection_config,\n",
    "                                    deployment_file=deployment_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884d8a2",
   "metadata": {},
   "source": [
    "The detector returns the variable **detections**, which is an ecosound annotation object. All detections can be founn by accessing **detections.data**. The start and stop times of each detection are defined by the fields **time_min_offset** and **time_max_offset**, respectively. The minimum and maximum frequency of the detection boxes are defined by the fields **frequency_min** and **frequency_min**, respectively.\n",
    "\n",
    "Here, we see that the algorithm found 34 detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc842ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at detection table\n",
    "detections.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b03bce",
   "metadata": {},
   "source": [
    "We can inspect the detections by ploting their time and frequency boxes on top of the spectrogram and waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433cfd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot detections\n",
    "graph_detec = tools.plot_single_channel(audio_files['path'][detection_config['AUDIO']['channel']],\n",
    "                                  detection_config['SPECTROGRAM']['frame_sec'],\n",
    "                                  detection_config['SPECTROGRAM']['window_type'],\n",
    "                                  detection_config['SPECTROGRAM']['nfft_sec'],\n",
    "                                  detection_config['SPECTROGRAM']['step_sec'],\n",
    "                                  detection_config['SPECTROGRAM']['fmin_hz'],\n",
    "                                  detection_config['SPECTROGRAM']['fmax_hz'],\n",
    "                                  detections=detections,\n",
    "                                  verbose=False)\n",
    "fig_graph_detec, ax_graph_detec = graph_detec.show(display=False)\n",
    "fig_graph_detec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbefead",
   "metadata": {},
   "source": [
    "## Step 5: Localizing the detected signals\n",
    "\n",
    "Now we can localize the detected signals using linearized inversion with data from all hydrophones. The code below may take several seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf689a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform localization using linearized invertion\n",
    "localizations = localization.LinearizedInversion.run_localization(audio_files, detections, deployment_info_file, detection_config, hydrophones_config, localization_config, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b012ce8",
   "metadata": {},
   "source": [
    "The localization results are in the **localizations** variable. It contains the same number of rows as the variable **detections**. Localization details can be found in **localizations.data**, and the localization fileds are as follows:\n",
    "* **x**, **y**, and **z** define the coordinates of the localizations\n",
    "* **x_err_low** and **x_err_high**, **y_err_low** and **y_err_high**,and **z_err_low** and **z_err_high** are the low and high localization uncertainties for each dimension.\n",
    "* **x_err_span**, **y_err_span**, and **z_err_span** define the span of the uncertainties in each dimension (i.e., x_err_span = x_err_high - x_err_low)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d683bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "localizations.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa398253",
   "metadata": {},
   "source": [
    "Note that some rows (i.e., detections) could not be localized (i.e., localization fields = NaNs) because the correlation value during the TDOA measurement process was too low, or because the linearized inversion process did not converge. There are also localizations that have large uncertainties that would not be very useful for matching with the video data.\n",
    "\n",
    "So, we can filter the localizations so we only keep the ones with an uncertainty span less than 1 m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ca9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that some detections with no localizations (nan) -> did not converge\n",
    "\n",
    "# Filter localization results to only keep results with low uncertainty\n",
    "localizations.filter(\"x_err_span < 1 & y_err_span < 1 & z_err_span < 1.5\", inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63299218",
   "metadata": {},
   "source": [
    "When we plot the signals that are remaining (after filtering the localizations with large uncertainties), we see that it kept all the fish sounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c444bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot spectrogram with measurements filtered\n",
    "graph_loc = tools.plot_single_channel(audio_files['path'][detection_config['AUDIO']['channel']],\n",
    "                                      detection_config['SPECTROGRAM']['frame_sec'],\n",
    "                                      detection_config['SPECTROGRAM']['window_type'],\n",
    "                                      detection_config['SPECTROGRAM']['nfft_sec'],\n",
    "                                      detection_config['SPECTROGRAM']['step_sec'],\n",
    "                                      detection_config['SPECTROGRAM']['fmin_hz'],\n",
    "                                      detection_config['SPECTROGRAM']['fmax_hz'],\n",
    "                                      detections=localizations,\n",
    "                                      verbose=False)\n",
    "fig_graph_loc, ax_graph_loc = graph_loc.show(display=False)\n",
    "fig_graph_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2f441",
   "metadata": {},
   "source": [
    "Finally, we can visualize the localizations (blue dots) with their unceratinties (red bars) in a 3D scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot localizations\n",
    "fig_loc, fig_ax = localization.plot_localizations3D(localizations=localizations, hydrophones=hydrophones_config)\n",
    "fig_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985a9c3",
   "metadata": {},
   "source": [
    "## Step 6: Saving the localization results\n",
    "The loclaization results can be saved to a csv file and/or a netcdf file that can be used later on by ecosound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv:\n",
    "localizations.to_csv('localization_results.csv')\n",
    "\n",
    "# save as netcdf\n",
    "localizations.data.drop(columns=['iterations_logs'],inplace=True)\n",
    "localizations.to_netcdf('localization_results.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
